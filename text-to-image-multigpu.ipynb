{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "# Qwen-Image å¤šGPUæ–‡æœ¬ç”Ÿæˆå›¾åƒæµ‹è¯•\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬ä½¿ç”¨å¤šGPUååŒå·¥ä½œæ¥è¿è¡ŒQwen-Imageæ¨¡å‹ï¼Œè§£å†³å•GPUæ˜¾å­˜ä¸è¶³çš„é—®é¢˜ã€‚\n",
    "\n",
    "## ç¡¬ä»¶è¦æ±‚\n",
    "- 2ä¸ªæˆ–æ›´å¤šGPUï¼ˆæ¯ä¸ªè‡³å°‘35GBæ˜¾å­˜ï¼‰\n",
    "- æ€»æ˜¾å­˜éœ€æ±‚ï¼šçº¦70GB\n",
    "\n",
    "## å¤šGPUå·¥ä½œåŸç†\n",
    "- æ¯ä¸ªGPUç‹¬ç«‹åŠ è½½ä¸€ä¸ªæ¨¡å‹å®ä¾‹\n",
    "- é€šè¿‡ä»»åŠ¡é˜Ÿåˆ—åˆ†é…å·¥ä½œè´Ÿè½½\n",
    "- æ”¯æŒå¹¶å‘å¤„ç†å¤šä¸ªå›¾åƒç”Ÿæˆä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef12562-dcce-459d-907c-ed294ad11db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modelscope ipywidgets tqdm accelerate diffusers==0.35.1 transformers gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0136b-ab36-41fd-9975-04a94dc30d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_download import download_model\n",
    "\n",
    "download_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20474daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"diffusers version:\", version(\"diffusers\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"transformers version:\", version(\"transformers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# æ£€æµ‹GPUé…ç½®\n",
    "if torch.cuda.is_available():\n",
    "    torch_dtype = torch.bfloat16\n",
    "    device = \"cuda\"\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU:\")\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "else:\n",
    "    torch_dtype = torch.float32\n",
    "    device = \"cpu\"\n",
    "    num_gpus = 0\n",
    "    print(\"æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPU\")\n",
    "\n",
    "print(f\"\\nä½¿ç”¨æ•°æ®ç±»å‹: {torch_dtype}\")\n",
    "print(f\"ä¸»è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multigpu-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®å¤šGPUç¯å¢ƒå˜é‡\n",
    "NUM_GPUS_TO_USE = min(2, num_gpus)  # æœ€å¤šä½¿ç”¨2ä¸ªGPU\n",
    "os.environ[\"NUM_GPUS_TO_USE\"] = str(NUM_GPUS_TO_USE)\n",
    "os.environ[\"TASK_QUEUE_SIZE\"] = \"10\"\n",
    "os.environ[\"TASK_TIMEOUT\"] = \"300\"\n",
    "\n",
    "print(f\"é…ç½®ä½¿ç”¨ {NUM_GPUS_TO_USE} ä¸ªGPU\")\n",
    "print(f\"ä»»åŠ¡é˜Ÿåˆ—å¤§å°: {os.environ['TASK_QUEUE_SIZE']}\")\n",
    "print(f\"ä»»åŠ¡è¶…æ—¶æ—¶é—´: {os.environ['TASK_TIMEOUT']}ç§’\")\n",
    "\n",
    "if NUM_GPUS_TO_USE < 1:\n",
    "    raise RuntimeError(\"éœ€è¦è‡³å°‘1ä¸ªGPUæ¥è¿è¡Œæ­¤ç¬”è®°æœ¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-multigpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¤šGPUç®¡ç†å™¨\n",
    "import sys\n",
    "import time\n",
    "sys.path.append('./src/examples')\n",
    "\n",
    "try:\n",
    "    from demo import MultiGPUManager\n",
    "    print(\"æˆåŠŸå¯¼å…¥å¤šGPUç®¡ç†å™¨\")\n",
    "except ImportError as e:\n",
    "    print(f\"å¯¼å…¥å¤šGPUç®¡ç†å™¨å¤±è´¥: {e}\")\n",
    "    print(\"è¯·ç¡®ä¿ src/examples/demo.py æ–‡ä»¶å­˜åœ¨\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-multigpu-manager",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–å¤šGPUç®¡ç†å™¨\n",
    "model_name = \"./models/Qwen-Image\"\n",
    "\n",
    "print(\"æ­£åœ¨åˆå§‹åŒ–å¤šGPUç®¡ç†å™¨...\")\n",
    "print(\"æ³¨æ„ï¼šè¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œå› ä¸ºæ¯ä¸ªGPUéƒ½éœ€è¦åŠ è½½å®Œæ•´çš„æ¨¡å‹\")\n",
    "\n",
    "gpu_manager = MultiGPUManager(\n",
    "    model_repo_id=model_name,\n",
    "    num_gpus=NUM_GPUS_TO_USE,\n",
    "    task_queue_size=10\n",
    ")\n",
    "\n",
    "# å¯åŠ¨GPUå·¥ä½œè¿›ç¨‹\n",
    "gpu_manager.start_workers()\n",
    "\n",
    "# ç­‰å¾…æ‰€æœ‰GPUåˆå§‹åŒ–å®Œæˆ\n",
    "print(\"ç­‰å¾…æ‰€æœ‰GPUåˆå§‹åŒ–å®Œæˆ...\")\n",
    "time.sleep(10)  # ç»™GPUä¸€äº›æ—¶é—´æ¥åˆå§‹åŒ–\n",
    "\n",
    "print(\"å¤šGPUç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-prompts-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®æµ‹è¯•å‚æ•°\n",
    "positive_magic = {\n",
    "    \"en\": \", Ultra HD, 4K, cinematic composition.\", # for english prompt\n",
    "    \"zh\": \", è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾.\" # for chinese prompt\n",
    "}\n",
    "\n",
    "# å®šä¹‰5ä¸ªæµ‹è¯•æç¤ºè¯\n",
    "test_prompts = [\n",
    "    {\n",
    "        \"name\": \"cyberpunk_city\",\n",
    "        \"prompt\": '''A photo of a cyberpunk city at night, with neon lights reflecting on the rain-slicked streets. A ramen stall's sign clearly reads \"æœªæ¥æ‹‰é¢\" (Future Ramen), with smaller text below saying \"AI-Powered Noodles.\" A nearby electronic billboard scrolls with binary code: \"01010100 01101111 01101110 01100111 01111001 01101001\".''',\n",
    "        \"description\": \"æµ‹è¯•éœ“è™¹ç¯æ•ˆæœä¸‹çš„ä¸­è‹±æ–‡æ–‡æœ¬æ¸²æŸ“ï¼Œä»¥åŠå¤æ‚èƒŒæ™¯ä¸‹æ•°å­—/ä»£ç çš„æ¸…æ™°åº¦\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ancient_book\",\n",
    "        \"prompt\": '''A close-up shot of a yellowed, ancient book open on a heavy wooden desk, with a quill pen beside it. On the page, beautiful calligraphy reads \"éœä»¥ä¿®èº«ï¼Œå„‰ä»¥é¤Šå¾·\" (Cultivate oneself in tranquility, nurture virtue through thrift). At the bottom of the page, a faded English note says: \"The virtues are cultivated in tranquility\".''',\n",
    "        \"description\": \"æµ‹è¯•ä¸åŒä¸­æ–‡ä¹¦æ³•é£æ ¼å’Œæ‰‹å†™æ–œä½“è‹±æ–‡çš„æ¸²æŸ“ï¼Œä»¥åŠæ–‡æœ¬åœ¨ç¾Šçš®çº¸ç­‰å¤è€æè´¨ä¸Šçš„çº¹ç†\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"product_packaging\",\n",
    "        \"prompt\": '''A high-end, close-up shot of a modern, minimalist skincare product box. The front of the box features the product name \"å…‰å­ç²¾å\" (Photon Essence) and the brand \"Quantum Glow.\" The ingredient list is clearly visible: \"Aqua, Glycerin, Peptide-5, Hyaluronic Acid (ç»å°¿é…¸)\".''',\n",
    "        \"description\": \"æµ‹è¯•ç°ä»£äº§å“è®¾è®¡ä¸Šå°åˆ·æ–‡æœ¬çš„æ¸…æ™°åº¦å’Œçº¹ç†ï¼Œè¯„ä¼°æ¨¡å‹å¤„ç†ä¸­è‹±æ··åˆæ’ç‰ˆã€å¤§å°å†™æ•æ„Ÿæ€§å’ŒæŠ€æœ¯æœ¯è¯­çš„èƒ½åŠ›\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"street_graffiti\",\n",
    "        \"prompt\": '''A weathered brick wall in a city alley, covered in colorful graffiti art. At the center of the graffiti is a large Chinese character \"é¾™\" (Dragon), with \"DRAGON'S BREATH\" written next to it in a spray-paint style font. In the corner, a small signature reads: \"By Artist Qwen, 2024\".''',\n",
    "        \"description\": \"æµ‹è¯•éæ ‡å‡†è‰ºæœ¯å­—ä½“ï¼ˆå¦‚å–·æ¼†å’Œæ‰‹å†™ï¼‰çš„æ¸²æŸ“ï¼Œä»¥åŠæ–‡æœ¬ä¸ç –å¢™ç²—ç³™çº¹ç†çš„èåˆæ•ˆæœ\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"code_screen\",\n",
    "        \"prompt\": '''A programmer's desk with a computer monitor displaying code. A Python function is clearly visible on the screen: \"def calculate_fibonacci(n): # è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—\\n    if n <= 1:\\n        return n\". A sticky note with the handwritten text \"Launch Deadline: 12/25\" is faintly visible in the screen's reflection.''',\n",
    "        \"description\": \"æµ‹è¯•ç­‰å®½å­—ä½“ï¼ˆå¦‚ä»£ç å­—ä½“ï¼‰çš„å‡†ç¡®æ¸²æŸ“ï¼ŒåŒ…æ‹¬ç¼©è¿›ã€è¯­æ³•é«˜äº®å’ŒåŒè¯­æ³¨é‡Šï¼Œä»¥åŠå±å¹•åå°„ç­‰å¤æ‚å…‰å­¦æ•ˆæœä¸‹çš„è¯¦ç»†æ–‡æœ¬å¤„ç†\"\n",
    "    }\n",
    "]\n",
    "\n",
    "negative_prompt = \" \" # Recommended if you don't use a negative prompt.\n",
    "\n",
    "# Generate with different aspect ratios\n",
    "aspect_ratios = {\n",
    "    \"1:1\": (1328, 1328),\n",
    "    \"16:9\": (1664, 928),\n",
    "    \"9:16\": (928, 1664),\n",
    "    \"4:3\": (1472, 1104),\n",
    "    \"3:4\": (1104, 1472),\n",
    "    \"3:2\": (1584, 1056),\n",
    "    \"2:3\": (1056, 1584),\n",
    "}\n",
    "\n",
    "width, height = aspect_ratios[\"16:9\"]\n",
    "\n",
    "print(f\"è®¾ç½®å®Œæˆï¼Œå°†ç”Ÿæˆ {len(test_prompts)} å¼ æµ‹è¯•å›¾ç‰‡\")\n",
    "print(f\"å›¾ç‰‡å°ºå¯¸: {width}x{height} (16:9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multigpu-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨å¤šGPUç”Ÿæˆå›¾ç‰‡\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"å¼€å§‹ä½¿ç”¨å¤šGPUç”Ÿæˆå›¾ç‰‡...\")\n",
    "print(f\"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "generation_results = []\n",
    "total_start_time = time.time()\n",
    "\n",
    "# ä¾æ¬¡ç”Ÿæˆ5å¼ å›¾ç‰‡\n",
    "for i, test_case in enumerate(test_prompts, 1):\n",
    "    print(f\"\\næ­£åœ¨ç”Ÿæˆç¬¬{i}å¼ å›¾ç‰‡: {test_case['name']}\")\n",
    "    print(f\"æµ‹è¯•ç›®æ ‡: {test_case['description']}\")\n",
    "    print(f\"æç¤ºè¯: {test_case['prompt'][:100]}...\")\n",
    "    \n",
    "    task_start_time = time.time()\n",
    "    \n",
    "    # ä½¿ç”¨GPUç®¡ç†å™¨æäº¤ä»»åŠ¡\n",
    "    result = gpu_manager.submit_task(\n",
    "        prompt=test_case['prompt'] + positive_magic[\"en\"],\n",
    "        negative_prompt=negative_prompt,\n",
    "        seed=42 + i,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        guidance_scale=4.0,\n",
    "        num_inference_steps=50,\n",
    "        timeout=300\n",
    "    )\n",
    "    \n",
    "    task_end_time = time.time()\n",
    "    task_duration = task_end_time - task_start_time\n",
    "    \n",
    "    if result['success']:\n",
    "        image = result['image']\n",
    "        gpu_id = result['gpu_id']\n",
    "        filename = f\"{test_case['name']}_multigpu_test.png\"\n",
    "        image.save(filename)\n",
    "        \n",
    "        generation_results.append({\n",
    "            'name': test_case['name'],\n",
    "            'filename': filename,\n",
    "            'gpu_id': gpu_id,\n",
    "            'duration': task_duration,\n",
    "            'success': True\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… å›¾ç‰‡å·²ä¿å­˜ä¸º: {filename}\")\n",
    "        print(f\"   ä½¿ç”¨GPU: {gpu_id}\")\n",
    "        print(f\"   ç”Ÿæˆæ—¶é—´: {task_duration:.2f}ç§’\")\n",
    "    else:\n",
    "        error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')\n",
    "        generation_results.append({\n",
    "            'name': test_case['name'],\n",
    "            'filename': None,\n",
    "            'gpu_id': None,\n",
    "            'duration': task_duration,\n",
    "            'success': False,\n",
    "            'error': error_msg\n",
    "        })\n",
    "        \n",
    "        print(f\"âŒ ç”Ÿæˆå¤±è´¥: {error_msg}\")\n",
    "        print(f\"   å°è¯•æ—¶é—´: {task_duration:.2f}ç§’\")\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_duration = total_end_time - total_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æ‰€æœ‰å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼\")\n",
    "print(f\"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"æ€»è€—æ—¶: {total_duration:.2f}ç§’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆç»“æœæ±‡æ€»\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆç»“æœæ±‡æ€»:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "successful_generations = [r for r in generation_results if r['success']]\n",
    "failed_generations = [r for r in generation_results if not r['success']]\n",
    "\n",
    "print(f\"æˆåŠŸç”Ÿæˆ: {len(successful_generations)}/{len(generation_results)} å¼ å›¾ç‰‡\")\n",
    "print(f\"å¤±è´¥æ•°é‡: {len(failed_generations)} å¼ \")\n",
    "\n",
    "if successful_generations:\n",
    "    avg_time = sum(r['duration'] for r in successful_generations) / len(successful_generations)\n",
    "    print(f\"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}ç§’\")\n",
    "    \n",
    "    # GPUä½¿ç”¨ç»Ÿè®¡\n",
    "    gpu_usage = {}\n",
    "    for r in successful_generations:\n",
    "        gpu_id = r['gpu_id']\n",
    "        if gpu_id not in gpu_usage:\n",
    "            gpu_usage[gpu_id] = 0\n",
    "        gpu_usage[gpu_id] += 1\n",
    "    \n",
    "    print(\"\\nGPUä½¿ç”¨åˆ†å¸ƒ:\")\n",
    "    for gpu_id, count in sorted(gpu_usage.items()):\n",
    "        print(f\"  GPU {gpu_id}: {count} å¼ å›¾ç‰‡\")\n",
    "\n",
    "print(\"\\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:\")\n",
    "for r in generation_results:\n",
    "    if r['success']:\n",
    "        print(f\"  âœ… {r['filename']} (GPU {r['gpu_id']}, {r['duration']:.1f}s)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {r['name']} - å¤±è´¥: {r.get('error', 'æœªçŸ¥é”™è¯¯')}\")\n",
    "\n",
    "if failed_generations:\n",
    "    print(\"\\nâš ï¸  å¤±è´¥çš„ä»»åŠ¡:\")\n",
    "    for r in failed_generations:\n",
    "        print(f\"  - {r['name']}: {r.get('error', 'æœªçŸ¥é”™è¯¯')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-resources",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†èµ„æº\n",
    "print(\"\\nğŸ§¹ æ­£åœ¨æ¸…ç†å¤šGPUèµ„æº...\")\n",
    "\n",
    "try:\n",
    "    gpu_manager.stop()\n",
    "    print(\"âœ… å¤šGPUç®¡ç†å™¨å·²åœæ­¢\")\n",
    "    print(\"âœ… æ‰€æœ‰GPUå·¥ä½œè¿›ç¨‹å·²ç»ˆæ­¢\")\n",
    "    print(\"âœ… èµ„æºæ¸…ç†å®Œæˆ\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {e}\")\n",
    "\n",
    "# æ¸…ç†CUDAç¼“å­˜\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… CUDAç¼“å­˜å·²æ¸…ç†\")\n",
    "\n",
    "print(\"\\nğŸ‰ å¤šGPUå›¾åƒç”Ÿæˆä»»åŠ¡å®Œæˆï¼\")\n",
    "print(\"\\nğŸ’¡ æç¤º:\")\n",
    "print(\"  - ç”Ÿæˆçš„å›¾ç‰‡ä¿å­˜åœ¨å½“å‰ç›®å½•\")\n",
    "print(\"  - æ–‡ä»¶åæ ¼å¼: {æµ‹è¯•åç§°}_multigpu_test.png\")\n",
    "print(\"  - å¦‚éœ€é‡æ–°è¿è¡Œï¼Œè¯·é‡å¯å†…æ ¸ä»¥é‡Šæ”¾GPUå†…å­˜\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-image (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
